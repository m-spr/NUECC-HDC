{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.system('pwd')\n",
    "\n",
    "import torch\n",
    "import torchhd\n",
    "from torchhd.models import Centroid\n",
    "from torchhd import embeddings\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchhd.datasets.isolet import ISOLET\n",
    "from torchhd.datasets import EMGHandGestures\n",
    "#from torch_geometric.datasets import TUDataset\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # classes = []\n",
    "# # modelload = \"MNISTmodels/MNIST\"+\"_\"+ str(Encoder)[17:-2]+\"_quantize_\"+str(q)+\"_\"+str(d)\n",
    "# # modelload1 = \"./\"+modelload+\".pt\"\n",
    "# # modelood = Centroid(d, num_classes)\n",
    "# # weights = torch.load( modelload1 , map_location=torch.device('cpu'))\n",
    "# # #print(weights[0])\n",
    "# # modelood.weight = weights\n",
    "# # #modelood.normalize(quantize=q)\n",
    "# # #print (\"modelood.weight after normalize  \", modelood.weight[4])\n",
    "# # #classes.append(modelood.weight.cpu().detach().numpy())\n",
    "# # #print (\"classes.  \", classes[0], len(classes) , len(classes[0]))\n",
    "# # weights = modelood.weight\n",
    "# # #print (\"k.  \", k[0], len(k) , len(k[0]))\n",
    "def extract_sparse_nonSparse_indexes(weights):\n",
    "    \"\"\"\n",
    "    Extracts sparse and non-sparse column indexes from a 2D list of weights.\n",
    "\n",
    "    Args:\n",
    "        weights (list of list of int/float): 2D list of weights.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "            - nonSparse: Indexes of columns with varying values.\n",
    "            - sparse: Indexes of columns with constant values.\n",
    "    \"\"\"\n",
    "    weights_list = weights.tolist()\n",
    "    nonSparse = []\n",
    "    sparse = []\n",
    "    num_rows = len(weights_list)\n",
    "\n",
    "    for col_idx in range(len(weights_list[0])):\n",
    "        # Get all values in the current column\n",
    "        column_values = [weights_list[row_idx][col_idx] for row_idx in range(num_rows)]\n",
    "\n",
    "        # Check if all values in the column are the same\n",
    "        if len(set(column_values)) == 1:\n",
    "            sparse.append(col_idx)\n",
    "        else:\n",
    "            nonSparse.append(col_idx)\n",
    "\n",
    "    return nonSparse, sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_rand(nn.Module):\n",
    "    def __init__(self, out_features, size, levels):\n",
    "        super(Encoder_rand, self).__init__()\n",
    "        #self.flatten = torch.nn.Flatten(start_dim=-2)\n",
    "        self.project = embeddings.Sinusoid(size, out_features)\n",
    "        self.name=\"RandomProjectionEncoder\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        sample_hv = self.project(x)\n",
    "        #sample_hv = torchhd.multiset(sample_hv)\n",
    "        return torchhd.hard_quantize(sample_hv)\n",
    "\n",
    "###### =====================================BASE LEVEL=============================================\n",
    "\n",
    "class Encoder_base(nn.Module):\n",
    "    def __init__(self, out_features, size, levels):\n",
    "        super(Encoder_base, self).__init__()\n",
    "        #self.flatten = torch.nn.Flatten(start_dim=-2)\n",
    "        self.position = embeddings.Random(size, out_features)\n",
    "        self.value = embeddings.Level(levels, out_features)\n",
    "        self.name=\"BaseLevelEncoder\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        sample_hv = torchhd.bind(self.position.weight, self.value(x))\n",
    "        sample_hv = torchhd.multiset(sample_hv)\n",
    "        return torchhd.hard_quantize(sample_hv)\n",
    "\n",
    "\n",
    "def inject_fault(classHyperVectors, bits, amount, mood):\n",
    "    \"\"\"\n",
    "    Injects faults into the specified columns (mood is the list of column that are going to be faulty) of classHyperVectors.\n",
    "\n",
    "    Args:\n",
    "        classHyperVectors: Object containing the weight data.\n",
    "        bits (int): Number of bits for the representation.\n",
    "        amount (int): Number of faults to inject.\n",
    "        mood (list): List of column indices where faults should be injected.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Modified tensor with injected faults.\n",
    "    \"\"\"\n",
    "    \n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # Extract the weights data\n",
    "    weight_data = classHyperVectors.weight.data.clone()\n",
    "    rows, cols = weight_data.shape\n",
    "\n",
    "    # Restrict fault injection to the mood columns\n",
    "    mood_cols = torch.tensor(mood, dtype=torch.long)\n",
    "    fault_index = rng.choice(\n",
    "        len(mood_cols) * rows * bits, amount, replace=False\n",
    "    )\n",
    "\n",
    "    # Flatten only the mood columns\n",
    "    mood_weights = weight_data[:, mood_cols].flatten()\n",
    "    if bits == 1:\n",
    "        mood_weights = F.relu(mood_weights).type(torch.int)\n",
    "    else:\n",
    "        mood_weights = mood_weights.type(torch.int)\n",
    "\n",
    "    # Convert to bitstring representation\n",
    "    bitstring = np.array(\n",
    "        list(\n",
    "            ''.join(\n",
    "                [format(weight & (2 ** bits - 1), '0' + str(bits) + 'b') for weight in mood_weights]\n",
    "            )\n",
    "        )\n",
    "    ).astype(int)\n",
    "\n",
    "    # Inject faults\n",
    "    bitstring[fault_index] = (bitstring[fault_index] - 1) * -1\n",
    "\n",
    "    # Convert back from bitstring\n",
    "    bitstring = bitstring.astype(str)\n",
    "    modified_weights = [\n",
    "        twos_comp(int(\"\".join(bitstring[i:i + bits]), 2), bits)\n",
    "        for i in range(0, len(bitstring), bits)\n",
    "    ]\n",
    "    modified_weights = torch.tensor(modified_weights).reshape(rows, len(mood_cols))\n",
    "\n",
    "    # Update the original tensor with modified values in the mood columns\n",
    "    result = weight_data.clone()\n",
    "    result[:, mood_cols] = modified_weights\n",
    "\n",
    "    if bits == 1:\n",
    "        result *= -1\n",
    "        result *= 2\n",
    "        result -= 1\n",
    "\n",
    "    return result\n",
    "\n",
    "def twos_comp(val, bits):\n",
    "    \"\"\"compute the 2's complement of int value val\"\"\"\n",
    "    if (val & (1 << (bits - 1))) != 0: # if sign bit is set e.g., 8bit: 128-255\n",
    "        val = val - (1 << bits)        # compute negative value\n",
    "    return val                         # return positive value as is\n",
    "\n",
    "def train_model(encoder, levels, d, size, num_classes, q, dataset, train_ld, test_ld, flatten = False, runs=10):\n",
    "    #train 10 times and take the best model (and encoder)\n",
    "    best_accuracy = 0\n",
    "    best_model = Centroid(d, num_classes)\n",
    "    if encoder == \"RandomProjectionEncoder\":\n",
    "        best_encoder = Encoder_rand(d, size, levels).to(device)\n",
    "    elif encoder ==\"BaseLevelEncoder\":\n",
    "        best_encoder = Encoder_base(d, size, levels).to(device)\n",
    "\n",
    "    for i in range(runs):\n",
    "        if encoder == \"RandomProjectionEncoder\":\n",
    "            encode = Encoder_rand(d, size, levels).to(device)\n",
    "        elif encoder ==\"BaseLevelEncoder\":\n",
    "            encode = Encoder_base(d, size, levels).to(device)\n",
    "            \n",
    "        model = Centroid(d, num_classes)\n",
    "        model = model.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for samples, labels in tqdm(train_ld, desc=\"Training\"):\n",
    "                samples = samples.to(device)\n",
    "                if flatten:\n",
    "                    samples = samples.flatten(start_dim=-2)\n",
    "                    samples = samples.reshape(samples.shape[0], samples.shape[-1])\n",
    "                labels = labels.to(device)\n",
    "                samples_hv = encode(samples)\n",
    "                model.add(samples_hv, labels)\n",
    "        \n",
    "        accuracy = torchmetrics.Accuracy(\"multiclass\", num_classes=num_classes)\n",
    "        dotsim = []\n",
    "        dotsim10 = []\n",
    "        classification = []\n",
    "        lb = []\n",
    "        with torch.no_grad():\n",
    "            model.min_max_normalize(quantize = q)\n",
    "\n",
    "            for samples, labels in tqdm(test_ld, desc=\"Testing\"):\n",
    "                samples = samples.to(device)\n",
    "                if flatten:\n",
    "                    samples = samples.flatten(start_dim=-2)\n",
    "                    samples = samples.reshape(samples.shape[0], samples.shape[-1])\n",
    "                samples_hv = encode(samples)\n",
    "                outputs = model(samples_hv, dot=True)\n",
    "                accuracy.update(outputs.cpu(), labels)\n",
    "                dotsim.extend(torch.max(outputs.cpu(),dim=1).values.tolist())\n",
    "                dotsim10.extend(np.sort(outputs.cpu().detach().numpy()))\n",
    "                classification.extend(torch.argmax(outputs.cpu(),dim=1).tolist())\n",
    "                lb.extend(labels.tolist())\n",
    "            dotsimbase = np.array(dotsim)\n",
    "            dotsimbase10 = np.array(dotsim10)\n",
    "            lb = np.array(lb)\n",
    "            classification = np.array(classification)\n",
    "    \n",
    "        #only keep the best model\n",
    "        current_accuracy = accuracy.compute().item() * 100\n",
    "        if current_accuracy > best_accuracy:\n",
    "            best_model.weight.data = model.weight.data.detach().clone()\n",
    "            best_encoder = encode\n",
    "            np.save(\"./models/\"+ dataset +\"/\" + encode.name + \"/normal/\" + \"quantize_\"+str(q)+\"_\"+str(d)+\"_\"+'dotsimbase10'+'.npy', dotsimbase10)\n",
    "            np.save(\"./models/\"+ dataset +\"/\" + encode.name + \"/normal/\" + \"quantize_\"+str(q)+\"_\"+str(d)+\"_\"+'dotsimbase'+'.npy', dotsimbase)\n",
    "            np.save(\"./models/\"+ dataset +\"/\" + encode.name + \"/normal/\" + \"quantize_\"+str(q)+\"_\"+str(d)+\"_\"+'classified'+'.npy', classification)\n",
    "            np.save(\"./models/\"+ dataset +\"/\" + encode.name + \"/normal/\" + \"quantize_\"+str(q)+\"_\"+str(d)+\"_\"+'labels'+'.npy', lb)\n",
    "            torch.save(model.weight, \"./models/\"+ dataset + \"/\" + encode.name + \"/\" + \"quantize_\" + str(q) + \"_\"+str(d) + \".pt\")\n",
    "            torch.save(encode, \"./models/\"+ dataset + \"/\" + encode.name + \"/\" + \"enc_\" + \"quantize_\" + str(q) + \"_\"+str(d) + \".pt\")\n",
    "    return best_model, best_encoder, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION = [1000, 5000, 10000]\n",
    "\n",
    "quantize = [1, 8]\n",
    "\n",
    "encoders = [\"RandomProjectionEncoder\", \"BaseLevelEncoder\"]\n",
    "\n",
    "averaging = [0,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISOLET\n",
    "\n",
    "# DIMENSIONS = 5000  # number of hypervector dimensions\n",
    "NUM_LEVELS = 100\n",
    "\n",
    "\n",
    "train_ds = ISOLET(\"data/\", train=True, download=False)\n",
    "train_ld = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_ds = ISOLET(\"data/\", train=False, download=False)\n",
    "test_ld = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "num_classes = len(train_ds.classes)\n",
    "\n",
    "for d in DIMENSION:\n",
    "    for q in quantize:\n",
    "        for Encoder in encoders:\n",
    "            \n",
    "            model, encode, accuracy = train_model(Encoder,\n",
    "                                        NUM_LEVELS,\n",
    "                                        d,\n",
    "                                        train_ds[0][0].size(-1),\n",
    "                                        num_classes,\n",
    "                                        q,\n",
    "                                        \"isolet\",\n",
    "                                        train_ld,\n",
    "                                        test_ld)\n",
    "\n",
    "            #inject fault\n",
    "            print(\"quantize_\"+str(q)+\"_\"+str(d))\n",
    "            print(f\"Testing accuracy of {(accuracy.compute().item() * 100):.3f}%\")\n",
    "            w = model.weight.data.detach().clone()\n",
    "            \n",
    "            for avg in range(averaging[0], averaging[1]):\n",
    "                df = pd.DataFrame(columns=[\n",
    "                'Fault %', 'Accuracy'\n",
    "                ])\n",
    "                faultyPoints = 0\n",
    "\n",
    "                while faultyPoints < d*num_classes*q:\n",
    "                    \n",
    "                    accuracies = []\n",
    "                    dotsim = []\n",
    "                    dotsim10 = []\n",
    "                    classification = []\n",
    "                    model_faulty = Centroid(d, num_classes)\n",
    "                    model_faulty = model_faulty.to(device)\n",
    "                    model_faulty.weight.data = w.clone()\n",
    "                    \n",
    "                    model_faulty.weight.data = inject_fault(model_faulty, amount=faultyPoints, bits=q).to(device)\n",
    "\n",
    "                    accuracy = torchmetrics.Accuracy(\"multiclass\", num_classes=num_classes)\n",
    "                    with torch.no_grad():\n",
    "                        # model.normalize(quantize = q)\n",
    "                        for samples, labels in tqdm(test_ld, desc=\"Testing\", disable= True):\n",
    "                            samples = samples.to(device)\n",
    "                            samples_hv = encode(samples)\n",
    "                            outputs = model_faulty(samples_hv, dot=True)\n",
    "                            accuracy.update(outputs.cpu(), labels)\n",
    "                            dotsim.extend(torch.max(outputs.cpu(),dim=1).values.tolist())\n",
    "                            dotsim10.extend(np.sort(outputs.cpu().detach().numpy()))\n",
    "                            classification.extend(torch.argmax(outputs.cpu(),dim=1).tolist())\n",
    "                    \n",
    "                    dotsimbase = np.array(dotsim)\n",
    "                    dotsimbase10 = np.array(dotsim10) \n",
    "                    classification = np.array(classification)\n",
    "\n",
    "                    np.save(\"./models/isolet/\" + encode.name + \"/faulty/\" + \"quantize_\"+str(q)+\"_\"+str(d)+\"_\"+str(avg)+\"_\"+\"dotsimbase10\"+\".npy\", dotsimbase10)\n",
    "                    np.save(\"./models/isolet/\" + encode.name + \"/faulty/\" + \"quantize_\"+str(q)+\"_\"+str(d)+\"_\"+str(avg)+\"_\"+\"dotsimbase\"+\".npy\", dotsimbase)\n",
    "                    np.save(\"./models/isolet/\" + encode.name + \"/faulty/\" + \"quantize_\"+str(q)+\"_\"+str(d)+\"_\"+str(avg)+\"_\"+\"classified\"+\".npy\", classification)\n",
    "\n",
    "                    row = {\n",
    "                            'Fault %': [faultyPoints/(d*num_classes*q)],\n",
    "                            'Accuracy': [accuracy.compute().item() * 100],\n",
    "                        }\n",
    "                    faultyPoints = int(faultyPoints + (d*num_classes*q)/100)\n",
    "                    row = pd.DataFrame.from_dict(row)\n",
    "                    df = pd.concat([df,row], ignore_index=True)\n",
    "                print(df)\n",
    "                df.to_excel(\"./models/isolet/\" + encode.name + \"/faulty/quantize_\"+str(q)+\"_\"+str(d)+\"_\"+str(avg)+\"result.xlsx\", index=False)               \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UCI_HAR_Dataset\n",
    "\n",
    "# Paths to the dataset\n",
    "train_data_path = 'data/UCI_HAR_Dataset/train/X_train.txt'\n",
    "train_labels_path = 'data/UCI_HAR_Dataset/train/y_train.txt'\n",
    "test_data_path = 'data/UCI_HAR_Dataset/test/X_test.txt'\n",
    "test_labels_path = 'data/UCI_HAR_Dataset/test/y_test.txt'\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(train_data_path, delim_whitespace=True, header=None)\n",
    "train_labels = pd.read_csv(train_labels_path, delim_whitespace=True, header=None)\n",
    "test_data = pd.read_csv(test_data_path, delim_whitespace=True, header=None)\n",
    "test_labels = pd.read_csv(test_labels_path, delim_whitespace=True, header=None)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "train_data_tensor = torch.tensor(train_data.values, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels.values.squeeze(), dtype=torch.long) - 1\n",
    "test_data_tensor = torch.tensor(test_data.values, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels.values.squeeze(), dtype=torch.long) - 1\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_ld = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_ld = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "NUM_LEVELS = 1000\n",
    "IMG_SIZE = 561\n",
    "num_classes = 7 #len(train_loader.classes)\n",
    "print(num_classes)\n",
    "\n",
    "for d in DIMENSION:\n",
    "    for q in quantize:\n",
    "        for Encoder in encoders:\n",
    "\n",
    "            model, encode, accuracy = train_model(Encoder,\n",
    "                                        NUM_LEVELS,\n",
    "                                        d,\n",
    "                                        IMG_SIZE,\n",
    "                                        num_classes,\n",
    "                                        q,\n",
    "                                        \"ucihar\",\n",
    "                                        train_ld,\n",
    "                                        test_ld)\n",
    "            \n",
    "            print(\"quantize_\"+str(q)+\"_\"+str(d))\n",
    "            print(f\"Testing accuracy of {(accuracy.compute().item() * 100):.3f}%\")\n",
    "\n",
    "            w = model.weight.data.detach().clone()\n",
    "            for avg in range(averaging[0], averaging[1]):\n",
    "                df = pd.DataFrame(columns=[\n",
    "                'Fault %', 'Accuracy'\n",
    "                ])\n",
    "                faultyPoints = 0\n",
    "                \n",
    "                while faultyPoints < d*num_classes*q:\n",
    "                    \n",
    "                    accuracies = []\n",
    "                    dotsim = []\n",
    "                    dotsim10 = []\n",
    "                    classification = []\n",
    "                    model_faulty = Centroid(d, num_classes)\n",
    "                    model_faulty = model_faulty.to(device)\n",
    "                    model_faulty.weight.data = w.clone()\n",
    "                    \n",
    "                    model_faulty.weight.data = inject_fault(model_faulty, amount=faultyPoints, bits=q).to(device)\n",
    "\n",
    "                    accuracy = torchmetrics.Accuracy(\"multiclass\", num_classes=num_classes)\n",
    "                    with torch.no_grad():\n",
    "                        # model.normalize(quantize = q)\n",
    "                        for samples, labels in tqdm(test_ld, desc=\"Testing\", disable= True):\n",
    "                            samples = samples.to(device)\n",
    "                            samples_hv = encode(samples)\n",
    "                            outputs = model_faulty(samples_hv, dot=True)\n",
    "                            accuracy.update(outputs.cpu(), labels)\n",
    "                            dotsim.extend(torch.max(outputs.cpu(),dim=1).values.tolist())\n",
    "                            dotsim10.extend(np.sort(outputs.cpu().detach().numpy()))\n",
    "                            classification.extend(torch.argmax(outputs.cpu(),dim=1).tolist())\n",
    "                    \n",
    "                    dotsimbase = np.array(dotsim)\n",
    "                    dotsimbase10 = np.array(dotsim10) \n",
    "                    classification = np.array(classification)\n",
    "\n",
    "                    np.save(\"./models/ucihar/\" + encode.name + \"/faulty/\" + \"quantize_\"+str(q)+\"_\"+str(d)+\"_\"+str(avg)+\"_\"+\"dotsimbase10\"+\".npy\", dotsimbase10)\n",
    "                    np.save(\"./models/ucihar/\" + encode.name + \"/faulty/\" + \"quantize_\"+str(q)+\"_\"+str(d)+\"_\"+str(avg)+\"_\"+\"dotsimbase\"+\".npy\", dotsimbase)\n",
    "                    np.save(\"./models/ucihar/\" + encode.name + \"/faulty/\" + \"quantize_\"+str(q)+\"_\"+str(d)+\"_\"+str(avg)+\"_\"+\"classified\"+\".npy\", classification)\n",
    "\n",
    "                    row = {\n",
    "                            'Fault %': [faultyPoints/(d*num_classes*q)],\n",
    "                            'Accuracy': [accuracy.compute().item() * 100],\n",
    "                        }\n",
    "                    faultyPoints = int(faultyPoints + (d*num_classes*q)/100)\n",
    "                    row = pd.DataFrame.from_dict(row)\n",
    "                    df = pd.concat([df,row], ignore_index=True)\n",
    "                print(df)\n",
    "                df.to_excel(\"./models/ucihar/\" + encode.name + \"/faulty/quantize_\"+str(q)+\"_\"+str(d)+\"_\"+str(avg)+\"result.xlsx\", index=False)               \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST dataset\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "\n",
    "train_ds = MNIST(\"data\", train=True, transform=transform, download=True)\n",
    "train_ld = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_ds = MNIST(\"data\", train=False, transform=transform, download=True)\n",
    "test_ld = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "NUM_LEVELS = 256\n",
    "IMG_SIZE = 28*28\n",
    "num_classes = 10 \n",
    "\n",
    "for d in DIMENSION:\n",
    "    for q in quantize:\n",
    "        for enc in encoders:\n",
    "            print(\"quantize_\"+str(q)+\"_\"+str(d))\n",
    "            model, encode, accuracy = train_model(enc,\n",
    "                                        NUM_LEVELS,\n",
    "                                        d,\n",
    "                                        IMG_SIZE,\n",
    "                                        num_classes,\n",
    "                                        q,\n",
    "                                        \"mnist\",\n",
    "                                        train_ld,\n",
    "                                        test_ld,\n",
    "                                        flatten=True,runs=10)\n",
    "            \n",
    "            print(f\"Testing accuracy of {(accuracy.compute().item() * 100):.3f}%\")\n",
    "\n",
    "            w = model.weight.data.detach().clone()\n",
    "            #inject fault\n",
    "            for avg in range(averaging[0], averaging[1]):\n",
    "                faultyPoints = 0\n",
    "                w = model.weight.data.detach().clone()\n",
    "                df = pd.DataFrame(columns=[\n",
    "                        'Fault %', 'Accuracy'\n",
    "                    ])\n",
    "                while faultyPoints < d*num_classes*q:\n",
    "\n",
    "                    accuracies = []\n",
    "                    dotsim = []\n",
    "                    dotsim10 = []\n",
    "                    classification = []\n",
    "                    model_faulty = Centroid(d, num_classes)\n",
    "                    model_faulty = model_faulty.to(device)\n",
    "                    model_faulty.weight.data = w.clone()\n",
    "                    model_faulty.weight.data = inject_fault(model_faulty, amount=faultyPoints, bits=q).to(device)\n",
    "                    \n",
    "                    accuracy = torchmetrics.Accuracy(\"multiclass\", num_classes=num_classes)\n",
    "                    with torch.no_grad():\n",
    "                        # model.normalize(quantize = q)\n",
    "                        for samples, labels in tqdm(test_ld, desc=\"Testing\", disable= True):\n",
    "                            samples = samples.to(device)\n",
    "                            samples = samples.flatten(start_dim=-2)\n",
    "                            samples_hv = encode(samples)\n",
    "                            outputs = model_faulty(samples_hv, dot=True)\n",
    "                            accuracy.update(torch.argmax(outputs.cpu(),dim=-1).flatten(), labels)\n",
    "                            dotsim.extend(torch.max(outputs.cpu(),dim=1).values.tolist())\n",
    "                            dotsim10.extend(np.sort(outputs.cpu().detach().numpy()))\n",
    "                            classification.extend(torch.argmax(outputs.cpu(),dim=1).tolist())\n",
    "                    \n",
    "                    dotsimbase = np.array(dotsim)\n",
    "                    dotsimbase10 = np.array(dotsim10)\n",
    "                    classification = np.array(classification)\n",
    "\n",
    "                    np.save(\"./models/mnist/\" + encode.name + \"/faulty/\" + \"quantize_\"+str(q)+\"_\"+str(d)+\"_\"+str(avg)+\"_\"+\"dotsimbase10\"+\".npy\", dotsimbase10)\n",
    "                    np.save(\"./models/mnist/\" + encode.name + \"/faulty/\" + \"quantize_\"+str(q)+\"_\"+str(d)+\"_\"+str(avg)+\"_\"+\"dotsimbase\"+\".npy\", dotsimbase)\n",
    "                    np.save(\"./models/mnist/\" + encode.name + \"/faulty/\" + \"quantize_\"+str(q)+\"_\"+str(d)+\"_\"+str(avg)+\"_\"+\"classified\"+\".npy\", classification)\n",
    "\n",
    "                    row = {\n",
    "                            'Fault %': [faultyPoints/(d*num_classes*q)],\n",
    "                            'Accuracy': [accuracy.compute().item() * 100],\n",
    "                        }\n",
    "                    faultyPoints = int(faultyPoints + (d*num_classes*q)/100)\n",
    "                    row = pd.DataFrame.from_dict(row)\n",
    "                    df = pd.concat([df,row], ignore_index=True)\n",
    "                print(df)\n",
    "                df.to_excel(\"./models/mnist/\" + encode.name + \"/faulty/quantize_\"+str(q)+\"_\"+str(d)+\"_\"+str(avg)+\"result.xlsx\", index=False)               \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
